<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>Efficient Inverse Reinforcement Learning without Compounding Errors</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<!-- <meta property="og:image" content="Path to my teaser.jpg"/> -->
  	<meta property="og:title" content="Efficient Inverse Reinforcement Learning without Compounding Errors" />
  	<meta property="og:description" content="Efficient Inverse Reinforcement Learning without Compounding Errors" />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Efficient Inverse Reinforcement Learning without Compounding Errors" />
    <meta property="twitter:description"   content="Efficient Inverse Reinforcement Learning without Compounding Errors" />
    <!-- <meta property="twitter:image"         content="Path to my teaser.jpg" /> -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        Efficient Inverse Reinforcement Learning <br> without Compounding Errors
    </div>

    <div class="venue">
        RLC 2024 RLSW and RLBRew Workshops
    </div>

    <br><br>

    <div class="author">
        <a href="https://nico-espinosadice.github.io/">Nicolas Espinosa Dice</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://wensun.github.io/">Wen Sun</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>Cornell University</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Carnegie Mellon University</div>

    <br><br>

    <div class="links"><a href="./static/efficient-irl.pdf">[Paper]</a></div>
    <!-- <div class="links"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">[Video]</a></div> -->
    <div class="links"><a href="https://github.com/nico-espinosadice/garage-fork/tree/main">[Code]</a></div>

    <br><br>

    <!-- <img style="width: 80%;" src="./resources/teaser.jpg" alt="Teaser figure."/> -->
    <!-- <br>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p> -->

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Inverse reinforcement learning (IRL) is an on-policy approach to imitation learning (IL) that allows the learner to observe the consequences of their actions at
            train-time. Accordingly, there are two seemingly contradictory desiderata for IRL
            algorithms: (a) preventing the compounding errors that stymie offline approaches
            like behavioral cloning and (b) avoiding the worst-case exploration complexity of
            reinforcement learning (RL). Prior work has been able to achieve either (a) or (b)
            but not both simultaneously. In our work, we first prove a negative result showing
            that, without further assumptions, there are no efficient IRL algorithms that avoid
            compounding errors in the worst case. We then provide a positive result: under a
            novel structural condition we term reward-agnostic policy completeness, we prove
            that efficient IRL algorithms do avoid compounding errors, giving us the best of
            both worlds. We also propose a principled method for using sub-optimal data to
            further improve the sample-efficiency of efficient IRL algorithms.
    </p>

    <br><br>
    <hr>

    <!-- <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div> -->

    <!-- <br><br>
    <hr> -->

    <!-- <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a> -->

    <!-- <br><br>
    <hr> -->

    <!-- <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.jpg"
         alt="Results figure"/>

    <br><br>
    <hr> -->

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="./static/efficient-irl.pdf">
            <img class="layered-paper-big" width="100%" src="./static/title-page.jpg" alt="Efficient Inverse Reinforcement Learning without Compounding Errors"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Efficient Inverse Reinforcement Learning without Compounding Errors</h3>
        <p>Nicolas Espinosa Dice, Gokul Swamy, Sanjiban Choudhury, Wen Sun</p>
        <p>In ICML 2024 MHFAIA Workshop.</p>
        <pre><code>@inproceedings{dice2024efficient,
            title={Efficient Inverse Reinforcement Learning without Compounding Errors},
            author={Dice, Nicolas Espinosa and Swamy, Gokul and Choudhury, Sanjiban and Sun, Wen},
            booktitle={ICML 2024 Workshop on Models of Human Feedback for AI Alignment}
}</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
